<%
  require "eruby_util.rb"
%>

<%
  chapter(
    '06',
    %q{Sources},
    'ch:sources'
  )
%>

<% begin_sec("Sources in general relativity") %>
<% begin_sec("Point sources in a background-independent theory") %>
Schr\"{o}dinger equation and Maxwell's equations treat spacetime as a stage on which particles and
fields act out their roles. General relativity, however, is essentially a theory of spacetime itself.
The role played by atoms or rays of light is so peripheral that by the time Einstein had derived
an approximate version of the Schwarzschild metric, and used it to find the precession of Mercury's
perihelion, he still had only vague ideas of how light and matter would fit into the picture. In his calculation,
Mercury played the role of a test particle: a lump of mass so tiny that it can be tossed into spacetime
in order to measure spacetime's curvature, without worrying about its effect on the spacetime, which is
assumed to be negligible. Likewise the sun was treated as in one of those orchestral pieces in which
some of the brass play from off-stage, so as to produce the effect of a second band heard from a distance.
Its mass appears simply as an adjustable parameter $m$ in the metric, and if we had never heard of the Newtonian
theory we would have had no way of knowing how to interpret $m$.

When Schwarzschild published his exact solution to the vacuum field equations, Einstein suffered from philosophical
indigestion. His strong belief in Mach's principle\index{Mach's principle} led him to believe that there was
a paradox implicit in an exact spacetime with only one mass in it. If Einstein's field equations were to mean
anything, he believed that they had to be interpreted in terms of the motion of one body relative to another.
In a universe with only one massive particle, there would be no relative motion, and so, it seemed to him,
no motion of any kind, and no meaningful interpretation for the surrounding spacetime.

Not only that, but Schwarzschild's solution had a singularity at its center. When a classical field theory
contains singularities, Einstein believed, it contains the seeds of its own destruction. As we've seen on page
\pageref{slime-and-socks}, this issue is still far from being resolved, a century later.

However much he might have liked to disown it, Einstein was now in possession of a solution to his field
equations for a point source. In a linear, background-dependent theory like electromagnetism, knowledge of such
a solution leads directly to the ability to write down the field equations with sources included. If Coulomb's
law tells us the $1/r^2$ variation of the electric field of a point charge, then we can infer Gauss's law.
The situation in general relativity is not this simple. The field equations of general relativity, unlike
the Gauss's law, are nonlinear, so we can't simply say that a planet or a star is a solution
to be found by adding up a large number of point-source solutions. It's also not clear how one could
represent a moving source, since the singularity is a point that isn't even part of the continuous structure
of spacetime (and its location is also hidden behind an event horizon, so it can't be observed from the outside).
<% end_sec %> % Point sources in a background-independent theory

<% begin_sec("The Einstein field equation") %>
<% begin_sec("The Einstein tensor") %>
Given these difficulties, it's not surprising that Einstein's first attempt at incorporating sources into
his field equation was a dead end. He postulated that the field equation would have the Ricci tensor on
one side, and the energy-momentum tensor $T^{ab}$ (page \pageref{sec:energy-momentum-tensor}) on the other,
\begin{equation*}
  R_{ab} = 8\pi T_{ab} \qquad ,
\end{equation*}
where a factor of $G/c^4$ on the right is suppressed by our choice of units, and the $8\pi$ is determined on
the basis of consistency with Newtonian gravity in the limit of weak fields and low velocities. The problem
with this version of the field equations can be demonstrated by counting variables. $R$ and $T$ are
symmetric tensors, so the field equation contains 10 constraints on the metric: 4 from the diagonal
elements and 6 from the off-diagonal ones. In addition, conservation of mass-energy requires the divergence-free
property $\nabla_b T^{ab}=0$,
because otherwise, for example, we could have a mass-energy tensor that varied as $T^{00}=kt$, describing
a region of space in which mass was uniformly appearing or disappearing at a constant rate. But this adds
4 more constraints on the metric, for a total of 14. The metric, however, is a symmetric rank-2 tensor itself,
so it only has 10 independent components. This overdetermination of the metric suggests that the proposed field equation
will not in general allow a solution to be evolved forward in time from a set of initial conditions given on
a spacelike surface, and this turns out to be true. It can in fact be shown that the only possible solutions
are those in which the traces $R=R\indices{^a_a}$ and $T=T\indices{^a_a}$ are constant throughout spacetime.

The solution is to replace $R_{ab}$ in the field equations with the a different tensor $G_{ab}$, called the
Einstein tensor,\index{Einstein tensor} defined by $G_{ab}=R_{ab}-(1/2)Rg_{ab}$,
\begin{equation*}
  G_{ab} = 8\pi T_{ab} \qquad .
\end{equation*}
The Einstein tensor is constructed exactly so that it is divergence-free, $\nabla_b G^{ab}=0$. (This is not obvious,
but can be proved by direct computation.) Therefore any energy-momentum tensor that satisfies the field equation
is automatically divergenceless, and thus no additional constraints need to be applied in order to guarantee
conservation of mass-energy.

Self-check: Does replacing $R_{ab}$ with $G_{ab}$ invalidate the Schwarzschild metric?
<% end_sec %> % The Einstein tensor
<% begin_sec("Further interpretation of the energy-momentum tensor") %>\label{sec:more-energy-mom-tensor}\index{energy-momentum tensor}
The energy-momentum tensor was briefly introduced in section \ref{sec:energy-momentum-tensor} on page \pageref{sec:energy-momentum-tensor}.
By applying the Newtonian limit of the field equation to the Schwarzschild metric, we find that
$T_{tt}$ is to be identified as the mass density. The Schwarzschild metric describes a spacetime using coordinates
in which the mass is at rest. In the cosmological applications we'll be considering shortly,
it also makes sense to adopt a frame of reference in which the local mass-energy is, on average, at rest,
so we can continue to think of $T_{tt}$ as the (average) mass density. By symmetry, $T$ must be diagonal in
such a frame. For example, if we had $T_{tx}\ne 0$, then the positive $x$ direction would be distinguished from
the negative $x$ direction, but there is nothing that would allow such a distinction. The spacelike components
are associated with the pressure, $P$. The form of the tensor with mixed upper and lower indices has the
simple form $T\indices{^\mu_\nu}=diag(-\rho,P,P,P)$.

<% end_sec %> % The energy-momentum tensor

<% begin_sec("The cosmological constant") %>\label{sec:cosmological-constant}% Is a sub-sub-section, so only refer to it by page, not by section number.

Having included the source term in the Einstein field equations, our most important application will be to cosmology.
Some of the relevant ideas originate long before Einstein.
Once Newton had formulated a theory of gravity as a universal attractive force, he realized that there would be a tendency for the universe
to collapse. He resolved this difficulty by assuming that the universe was infinite in spatial extent, so that it would have
no center of symmetry, and therefore no preferred point to collapse toward. The trouble with this argument is that the
equilibrium it describes is unstable. Any perturbation of the uniform density of matter breaks the symmetry, leading to
the collapse of some pocket of the universe. If the radius of such a collapsing region is $r$, then its gravitational is
proportional to $r^3$, and its gravitational field is proportional to $r^3/r^2=r$. Since its acceleration is proportional
to its own size, the time it takes to collapse is independent of its size. The prediction is that the universe will
have a self-similar structure, in which the clumping on small scales behaves in the same way as clumping on large scales;
zooming in or out in such a picture gives a landscape that appears the same. With modern hindsight, this is actually not
in bad agreement with reality. We observe that the universe has a hierarchical structure consisting of solar systems,
galaxies, clusters of galaxies, superclusters, and so on. Once such a structure starts to condense, the collapse tends
to stop at some point because of conservation of angular momentum. This is what happened, for example, when our own
solar system formed out of a cloud of gas and dust.

Einstein confronted similar issues, but in a more acute form. Newton's symmetry argument, which failed only because of
its instability, fails even more badly in relativity: the entire spacetime can simply contract uniformly over time,
without singling out any particular point as a center. Furthermore, it is not obvious that angular momentum
prevents total collapse in relativity in the same way that it does classically, and even if it did, how would
that apply to the universe as a whole? Einstein's Machian orientation would have led him to reject the idea that
the universe as a whole could be in a state of rotation, and in any case it was sensible to start the study of
relativistic cosmology with the simplest and most symmetric possible models, which would have no preferred axis
of rotations.

Because of these issues, Einstein decided to try to patch up his field equation so that it would allow a static
universe.
Looking back over the considerations that led us to this form of the equation, we see that it is
very nearly uniquely determined by the following criteria:
\begin{itemize}
\item The equivalence principle is satisfied.
\item It should be coordinate-independent.
\item It should be equivalent to Newtonian gravity in the appropriate limit.
\item It should not be overdetermined.
\end{itemize}
This is not meant to be a rigorous proof, just a general observation that it's not easy to tinker with the
theory without breaking it.

\begin{eg}{A failed attempt at tinkering}
As an example of the lack of ``wiggle room'' in the structure of the field equations, suppose we
construct the scalar $T\indices{^a_a}$, the trace of the energy-momentum tensor, and try to insert
it into the field equations as a further source term. The first problem is that the field equation
involves rank-2 tensors, so we can't just add a scalar. To get around this, suppose we multiply by the
metric. We then have something like $G_{ab} = c_1 T_{ab}+c_2 g_{ab} T\indices{^c_c}$, where
the two constants $c_1$ and $c_2$ would be constrained by the requirement that the theory agree
with Newtonian gravity in the classical limit.

This particular attempt fails, because it violates the equivalence principle. Consider a beam of
light directed along the $x$ axis. Its momentum is equal to its energy (see page \pageref{sec:momentum-four-vector}),
so its contributions to the local energy density and pressure are equal.
Thus its contribution to the energy-momentum tensor is of the form $T\indices{^\mu_\nu}=(\text{constant})\times diag(-1,1,0,0)$.
The trace vanishes, so its coupling to gravity in the $c_2$ term is zero. But this violates the equivalence
principle, which requires that all forms of mass-energy contibute equally to gravitational mass.
\end{eg}

One way in which we \emph{can} change the field equation without violating any of these is to add a term $\Lambda g_{ab}$, giving
\begin{equation*}
  G_{ab} = 8\pi T_{ab} + \Lambda g_{ab} \qquad ,
\end{equation*}
which is what we will refer to as the Einstein field equation.\footnote{In books that use a $-+++$ metric rather then our
$+---$, the sign of the cosmological constant term is reversed relative to ours.}
The universal constant $\Lambda$ is called the cosmological constant.\index{cosmological constant}\index{Einstein field equation}
Einstein originally introduced a positive cosmological constant because he wanted relativity to be able to describe a static universe.
To see why it would have this effect, compare its behavior with that of an ordinary fluid.
When an ordinary fluid, such as the exploding air-gas mixture in a car's cylinder, expands, it does work on its environment,
and therefore by conservation of energy its own internal energy is reduced. A positive cosmological constant,
however, acts like a certain amount of mass-energy built into every cubic meter of vacuum. Thus when it
expands, it \emph{releases} energy. Its pressure is negative.

Now consider the following pseudo-classical argument.
Although we've already seen (page \pageref{no-separate-k-and-u}) that there is no useful way to separate the
roles of kinetic and potential energy in general relativity, suppose that there are some quantities analogous
to them in the description of the universe as a whole. (We'll see below that the universe's contraction and
expansion is indeed described by a set of differential equations that can be interpreted in essentially this way.)
If the universe contracts, a cubic meter of space becomes less than a cubic meter.
The cosmological-constant energy associated with that volume is reduced, so some energy has been consumed.
The kinetic energy of the collapsing matter goes down, and the collapse is decelerated.

The addition of the $\Lambda$ term constitutes a change to the \emph{vacuum} field equations, and the good agreement
between theory and experiment in the case of, e.g., Mercury's orbit puts an upper limit on $\Lambda$ then implies
that $\Lambda$ must be small. For an
order-of-magnitude estimate, consider that $\Lambda$ has units of mass density, and the only parameters with units
that appear in the description of Mercury's orbit are the mass of the sun, $m$, and the radius of Mercury's orbit, $r$.
The relativistic corrections to Mercury's orbit are on the order of $v^2$, or about $10^{-8}$, and the come out right.
Therefore we can estimate  that the cosmological constant could not have been greater than about $(10^{-8})m/r^3 \sim 10^{-10}\ \kgunit/\munit^3$,
or it would have caused noticeable discrepancies. This is a very poor bound; if $\Lambda$ was this big, we might even be
able to detect its effects in laboratory experiments.
Looking at the role played by $r$ in the estimate, we see that the
upper bound could have been made tighter by increasing $r$. Observations on galactic scales, for example, constrain it much
more tightly. This justifies the description of $\Lambda$ as cosmological:
the larger the scale, the more significant the effect of a nonzero $\Lambda$ would be.

<% end_sec %> % The cosmological constant
<% end_sec %> % The Einstein field equation
<% end_sec %> % Sources in general relativity

<% begin_sec("Cosmological solutions") %>\label{sec:cosmological-solutions}
Motivated by Hubble's observation that the universe is expanding, we hypothesize the existence of solutions of the
field equation in which the properties of space are isotropic (the same in all spatial directions) and homogeneous (the same at all
locations in space), but the over-all scale of space is increasing as described by some scale function $a(t)$. Because of
coordinate invariance, the metric can still be written in a variety of forms. One such form is
\begin{equation*}
  \der s^2 = \der t^2 - a(t)\der\ell^2 \qquad ,
\end{equation*}
where the spatial part is
\begin{equation*}
  \der\ell^2 = f(r)\der r^2 + r^2 \der\theta^2 + r^2 \sin^2\theta \der\phi^2 \qquad .
\end{equation*}
In these coordinates, the time $t$ is interpreted as the proper time of a particle that has always been at rest. Events
that are simultaneous according to this $t$ are events at which the local properties of the universe --- i.e., its curvature ---
are the same. These coordinates are referred as the ``standard'' cosmological coordinates; one will also encounter
other choices, such as the comoving and conformal coordinates, which are more convenient for certain purposes.
Historically, the solution for the functions $a$ and $f$ was found by de 
Sitter in 1917.\index{de Sitter}\index{standard cosmological coordinates}\index{comoving cosmological coordinates}\index{conformal cosmological coordinates}
\index{cosmological coordinates!standard}\index{cosmological coordinates!comoving}\index{cosmological coordinates!conformal}\label{standard-cosm-coords}

The unknown function $f(r)$ has to make the 3-space metric $\der\ell^2$ have a constant Einstein curvature tensor.
The following Maxima program computes the curvature.
\begin{listing}{1}
load(ctensor);
dim:3;
ct_coords:[r,theta,phi];
depends(f,t);
lg:matrix([f,0,0],
          [0,r^2,0],
          [0,0,r^2*sin(theta)^2]);
cmetric();
einstein(true);
\end{listing}
% constant_curvature.mac
Line 2 tells Maxima that we're working in a space with three dimensions rather than its default of four.
Line 4 tells it that $f$ is a function of time.  Line 9 uses its built-in function for computing the Einstein tensor $G\indices{^a_b}$. The result has only one nonvanishing
component, $G\indices{^t_t}=(1-1/f)/r^2$. This has to be constant, and since scaling can be absorbed in the factor $a(t)$ in the
3+1-dimensional metric, we can just set the value of $G_{tt}$ more or less arbitrarily, except for its sign.
The result is $f=1/(1-kr^2)$, where $k=-1$, 0, or 1. The form of $\der\ell^2$ shows us that $k$ can be interpreted
in terms of the sign of the spatial curvature. The $k=0$ case gives a flat space. For negative $k$, a circle of radius $r$ centered
on the origin has a circumference $2\pi r f(r)$ that is less than its Euclidean value of $2\pi r$. The opposite occurs for
$k>0$.
The resulting metric, called the Robertson-Walker metric, is
\begin{equation*}
  \der s^2 = \der t^2 - a^2\left(\frac{\der r^2}{1-kr^2} + r^2 \der\theta^2 + r^2 \sin^2\theta \der\phi^2\right) \qquad .
\end{equation*}

Having fixed $f(r)$, we can now see what the field equation tells us about $a(t)$.
The next program computes the Einstein tensor for the full four-dimensional spacetime:
\begin{listing}{1}
load(ctensor);
ct_coords:[t,r,theta,phi];
depends(a,t);
lg:matrix([1,0,0,0],
          [0,-a^2/(1-k*r^2),0,0],
          [0,0,-a^2*r^2,0],
          [0,0,0,-a^2*r^2*sin(theta)^2]);
cmetric();
einstein(true);
\end{listing}
% cosmology.mac
The result is
\begin{align*}
  G\indices{^t_t} &= 3\left(\frac{\dot{a}}{a}\right)^2 + 3ka^{-2} \\
  G\indices{^r_r} = G\indices{^\theta_\theta} = G\indices{^\phi_\phi} &= 2\frac{\ddot{a}}{a} + \left(\frac{\dot{a}}{a}\right)^2 + ka^{-2} \qquad ,
\end{align*}
where dots indicate differentiation with respect to time.

Since we have $G\indices{^a_b}$ with mixed upper and lower indices, we either have to convert it into $G_{ab}$, or write out the
field equations in this mixed form. The latter turns out to be simpler. In terms of mixed indices, $g\indices{^a_b}$ is always simply
$diag(1,1,1,1)$. Arbitrarily singling out $r=0$ for simplicity, we have $g=diag(1,-a^2,0,0)$. The energy-momentum tensor is
$T\indices{^\mu_\nu}=diag(-\rho,P,P,P)$.
Substituting into $G\indices{^a_b}=8\pi T\indices{^a_b}+\Lambda g\indices{^a_b}$, we find
\begin{align*}
  3\left(\frac{\dot{a}}{a}\right)^2 + 3ka^{-2} - \Lambda                     &= 8\pi\rho \\
  2\frac{\ddot{a}}{a} + \left(\frac{\dot{a}}{a}\right)^2 + ka^{-2} - \Lambda &= -8\pi P \qquad .
\end{align*}
Rearranging a little, we have the Friedmann equations,\index{Friedmann equations}
\begin{align*}
  \frac{\ddot{a}}{a}   \quad          &= \frac{1}{3}\Lambda - \frac{4\pi}{3}(\rho+3P) \\
  \left(\frac{\dot{a}}{a}\right)^2    &= \frac{1}{3}\Lambda + \frac{8\pi}{3}\rho-k a^{-2} \qquad .
\end{align*}

% This now checks out against Carroll, \url{http://nedwww.ipac.caltech.edu/level5/March01/Carroll3/Carroll8.html }, except for the
%       lambda terms, which he doesn't include.

<% begin_sec("Evidence for expansion of the universe") %>\label{evidence-for-expansion}
By 1929, Edwin Hubble\index{Hubble, Edwin} at Mount Wilson had determined that the universe was expanding rather than static, so
that Einstein's original goal of allowing a static cosmology became pointless. The universe, it seemed, had
originated in a Big Bang\index{Big Bang} (a concept that originated with the Belgian Roman Catholic priest
Georges Le\-ma\^{i}tre).
This now appears natural, since the Friedmann equations would only
allow a constant $a$ in the case where $\Lambda$ was perfectly tuned relative to the other parameters. 
Einstein later referred
to the cosmological constant as the ``greatest blunder of my life,'' and for the next 70 years it was
commonly assumed that $\Lambda$ was exactly zero.

Self-check: Why is it not correct to think of the Big Bang as an explosion that occurred at a specific
point in space?

The existence of the Big Bang is confirmed directly by looking up in the sky and seeing it.
In 1964, Penzias and Wilson\index{Penzias, Arno}\index{Wilson, Robert} at Bell Laboratories in New Jersey detected a mysterious background of microwave radiation
using a directional horn antenna. As with many accidental
discoveries in science, the important thing was to pay attention to the surprising observation
rather than giving up and moving on when it confounded attempts to understand it. They pointed
the antenna at New York City, but the signal didn't increase. The radiation didn't
show a 24-hour periodicity, so it couldn't be from a source in a certain direction in the sky.
They even went so far as to sweep out the pigeon droppings inside.
It was eventually established that the radiation was
coming uniformly from all directions in the
sky and had a black-body spectrum with a temperature of about 3 K.

This is now interpreted as follows. Soon after the Big Bang, the universe was hot enough to
ionize matter. An ionized gas is opaque to light, since the oscillating fields of an electromagnetic
wave accelerate the charged particles, depositing kinetic energy into them. Once the universe
became cool enough, however, matter became electrically neutral, and the universe became
transparent. Light from this time is the most long-traveling light that we can detect now.
The latest data show that transparency set in around $4\times 10^5$ years after the big bang,
when the temperature was about 3000 K. The surface we see, dating back to this time, is known
as the surface of last scattering.\index{surface of last scattering} Since then, the universe has expanded by about a factor
of 1000, causing the wavelengths of photons to be stretched by the same amount due to the
expansion of the underlying space. This is equivalent to a Doppler shift due to the source's
motion away from us; the two explanations are equivalent. We therefore see the 3000 K optical
black-body radiation red-shifted to 3 K, in the microwave region.

<% end_sec %> % evidence for expansion of the universe

<% begin_sec("Observability of expansion") %>\label{sec:observability-of-expansion}
The proper interpretation of the expansion of the universe, as described by the Friedmann equations,
can be tricky. It might seem as though the expansion would be undetectable, in the sense that
general relativity is coordinate-independent, and therefore does not pick out any preferred distance scale.
That is, if all our meter-sticks expand, and the rest of the universe expands as well, we would
have no way to detect the expansion. The flaw in this reasoning is that the Friedmann equations
only describe the average behavior of spacetime. As dramatized in the classic Woody Allen movie
``Annie Hall:'' ``Well, the universe is everything, and if it's expanding, someday it will break apart and that would be the end of everything!''
``What has the universe got to do with it? You're here in Brooklyn! Brooklyn is not expanding!''

One way to see that the expansion does not apply
on every scale would be to solve the Einstein field equations exactly so as to describe the
internal structure of the bodies that occupy the space: galaxies, superclusters, etc.
This is impractical in general, but can be done in simple cases, as in example
\ref{eg:sch-de-sitter} on page \pageref{eg:sch-de-sitter}.

Another way to see this is that if meter-sticks expanded along with the universe,
then the expansion would be nothing more than a change of coordinates. But the Ricci and
Einstein tensors were carefully constructed so as to be intrinsic. The fact that the expansion
affects the Einstein tensor shows that it cannot interpreted as a mere coordinate expansion.

So in general a gravitationally bound system does not
expand due to the stretching of the cosmological metric, nor does a system bound by electrical
or nuclear forces. Note that this is different from the case of a photon traveling across the
universe as described above. Such a photon \emph{does} expand, and this, too, is required
by the correspondence principle. If the photon did not expand, then its wavelength would
remain constant, and this would be inconsistent with the classical theory of
electromagnetism, which predicts a Doppler shift due to the relative motion of the
source and the observer.

<% end_sec %> % Observability of expansion

<% begin_sec("The vacuum-dominated solution") %>\label{sec:vacuum-dominated}
But observations of distant supernovae starting around 1998 introduced a further twist in the plot.
In a binary star system consisting of a white dwarf and a non-degenerate star, as the non-degenerate
star evolves into a red giant, its size increases, and it can begin dumping mass onto the white dwarf.
This can cause the white dwarf to exceed the Chandrasekhar limit (page \pageref{chandrasekhar-limit}),
resulting in an explosion known as a type Ia supernova. Because the Chandrasekhar limit provides a uniform
set of initial conditions, the behavior of type Ia supernovae is fairly predictable, and in particular
their luminosities are approximately equal. They therefore provide a kind of standard candle: since
the intrinsic brightnes is known, the distance can be inferred from the apparent brightness. Given the
distance, we can infer the time that was spent in transit by the light on its way to us, i.e. the
look-back time. From measurements of Doppler shifts of spectral lines, we can also find the velocity
at which the supernova was receding from us. The result is that we can measure the universe's
rate of expansion as a function of time. Observations show that this rate of expansion has been
accelerating. The Friedmann equations show that this can only occur for $\Lambda \gtrsim 4\rho$.
This picture has been independently verified by measurements of the cosmic microwave background
radiation.\index{cosmic microwave background}

With hindsight, we can see that
in a quantum-mechanical context, it is natural to expect that fluctuations of the vacuum, required by the Heinsenberg
uncertainty principle, would contribute to the cosmological constant, and in fact models tend to overpredict $\Lambda$
by a factor of about $10^{120}$! From this point of view, the mystery is why these effects cancel out so precisely.
A correct understanding of the cosmological constant presumably requires a full theory
of quantum gravity, which is presently far out of our reach.

The latest data show that our universe, in the present epoch, is dominated by the cosmological constant,
so as an approximation we can write the Friedmann equations as
\begin{align*}
  \frac{\ddot{a}}{a}   \quad          &= \frac{1}{3}\Lambda  \\
  \left(\frac{\dot{a}}{a}\right)^2    &= \frac{1}{3}\Lambda  \qquad .
\end{align*}
This is referred to as a vacuum-dominated universe.
The solution is
\begin{equation*}
  a = \exp\left[\sqrt{\frac{\Lambda}{3}} \: t\right] \qquad .
\end{equation*}
The implications for the fate of the universe are depressing. All parts of the universe will accelerate
away from one another faster and faster as time goes on. The relative separation between two objects, say galaxy A
and galaxy B, will eventually be increasing faster than the speed of light. (The Lorentzian character of spacetime
is local, so relative motion faster than $c$ is only forbidden between objects that are passing right by one another.)
At this point, an observer in either galaxy will say that the other one has passed behind an event horizon.
If intelligent observers do actually exist in the far future, they may have no way to tell that the cosmos even exists.
They will perceive themselves as living in island universes, such as we believed our own galaxy to be a hundred years ago.

When I introduced the standard cosmological coordinates on page \pageref{standard-cosm-coords}, I described them
as coordinates in which events that are simultaneous according to this $t$ are events at which the local properties of the universe are the same.
In the case of a perfectly vacuum-dominated universe, however, this notion loses its meaning. The only observable
local property of such a universe is the vacuum energy described by the cosmological constant, and its density is
always the same, because it is built into the structure of the vacuum. Thus the vacuum-dominated cosmology is
a special one that maximally symmetric, in the sense that it has not only the symmetries of homogeneity and
isotropy that we've been assuming all along, but also a symmetry with respect to time: it is a cosmology without
history, in which all times appear identical to a local oberver. In the special case of this cosmology, the
time variation of the scaling factor $a(t)$ is unobservable, and may be thought of as the unfortunate result
of choosing an inappropriate set of coordinates, which obscure the underlying symmetry. When I argued in section
\ref{sec:observability-of-expansion} for the observability of the universe's expansion, note that all my
arguments assumed the presence of matter or radiation. These are completely absent in a perfectly
vacuum-dominated cosmology.

For these reasons de Sitter originally proposed this solution as a static universe in 1927. But by 1920
it was realized that this was an oversimplification. The argument above only shows that the time variation
of $a(t)$ does not allow us to distinguish one epoch of the universe from another. That is, we can't
look out the window and infer the date (e.g., from the temperature of the cosmic microwave background
radiation). It does not, however, imply that the universe is static in the
sense that had been assumed until Hubble's observations.
The $r$-$t$ part of the metric is
\begin{equation*}
  \der s^2 = \der t^2 - a^2 \der r^2 \qquad ,
\end{equation*}
where $a$ blows up exponentially with time, and the $k$-dependence has been
neglected, as it was in the approximation to the Friedmann equations
used to derive $a(t)$.\footnote{A computation of the Einstein tensor with $\der s^2 = \der t^2 - a^2(1-kr^2)^{-1} \der r^2$ shows
that $k$ enters only via a factor the form $(\ldots)e^{(\ldots)t}+(\ldots)k$. For large $t$, the $k$ term
becomes negligible, and the Einstein tensor becomes $G\indices{^a_b}=g\indices{^a_b}\Lambda$,
This is consistent with the approximation we used in deriving the solution, which was to
ignore both the source terms and the $k$ term in the Friedmann equations. The exact solutions with $\Lambda>0$ and
$k=-1$, 0, and 1 turn out in fact to be equivalent except for a change of coordinates.}
Let a test particle travel in the radial direction, starting at event $\zu{A}=(0,0)$ and ending at
$\zu{B}=(t',r')$. In flat space, a world-line of the linear form $r=vt$ would be a geodesic connecting
A and B; it would maximize the particle's proper time.
But in the this metric, it cannot be a geodesic. The curvature of geodesics relative to a line on
an $r$-$t$ plot is most easily understood in the limit where $t'$ is fairly long compared to the
time-scale $T=\sqrt{3/\Lambda}$ of the exponential, so that $a(t')$ is huge. The particle's best strategy for maximizing
its proper time is to make sure that its $\der r$ is extremely small when $a$ is extremely large.
The geodesic must therefore have nearly constant $r$ at the end. This makes it sound as though the
particle was decelerating, but in fact the opposite is true. If $r$ is constant, then the particle's
spacelike distance from the origin is just $r a(t)$, which blows up exponentially. The near-constancy of
the coordinate $r$ at large $t$ actually means that the particle's motion at large $t$ isn't really
due to the particle's inertial memory of its original motion, as in Newton's first law. What happens
instead is that the particle's initial motion allows it to move some distance away from the origin during a
time on the order of $T$, but after that, the expansion of the universe has become so rapid that the
particle's motion simply streams outward because of the expansion of space itself. Its initial motion only
mattered because it determined how far out the particle got before being swept away by the exponential expansion.

\begin{eg}{Geodesics in a vacuum-dominated universe}\label{eg:geodesic-vacuum-dominated}
In this example we confirm the above interpretation in the special case where the particle, rather
than being released in motion at the origin, is released at some nonzero radius $r$, with $\der r/\der t=0$ initially.
First we recall the geodesic equation
\begin{equation*}
  \frac{\der^2 x^i}{\der\lambda^2} = \Gamma\indices{^i_{jk}} \frac{\der x^j}{\der\lambda} \frac{\der x^k}{\der\lambda} \qquad .
\end{equation*}
from page \pageref{geodesic-diffeq}. The nonvanishing Christoffel symbols for the 1+1-dimensional metric 
$\der s^2 = \der t^2 - a^2 \der r^2$ are $\Gamma\indices{^r_{tr}}=\dot{a}/a$ and $\Gamma\indices{^t_{rr}}=-\dot{a}a$.
Setting $T=1$ for convenience, we have $\Gamma\indices{^r_{tr}}=1$ and $\Gamma\indices{^t_{rr}}=-e^{-2t}$.
% geodesic_vacuum_dominated.mac

We conjecture that the particle remains at the same value of $r$. Given this conjecture, the particle's
proper time $\int \der s$ is simply the same as its time coordinate $t$, and we can therefore use $t$ as
an affine coordinate.
Letting $\lambda=t$, we have
\begin{align*}
  & \frac{\der^2 t}{\der t^2}-\Gamma\indices{^t_{rr}}\left(\frac{\der r}{\der t}\right)^2 = 0\\
  &0-\Gamma\indices{^t_{rr}}\dot{r}^2 = 0 \\
  & \dot{r} = 0 \\
  & r = \text{constant}
\end{align*}
This confirms the self-consistency of the conjecture that $r=\text{constant}$ is a geodesic.

Note that we never actually had to use the actual expressions for the Christoffel symbols; we only needed to know
which of them vanished and which didn't. The conclusion
depended only on the fact that the metric had the form $\der s^2 = \der t^2 - a^2 \der r^2$ for some function
$a(t)$. This provides a rigorous justification for the interpretation of the cosmological scale factor $a$
as giving a universal time-variation on all distance scales.

The calculation also confirms that there is nothing special about $r=0$. A particle released with $r=0$ and $\dot{r}=0$
initially stays at $r=0$, but a particle released at any other value of $r$ also stays at that $r$.
This cosmology is homogeneous, so any point could have been
chosen as $r=0$. If we sprinkle test particles, all at rest, across the surface of a sphere centered on this arbitrarily
chosen point, then they will all accelerate outward \emph{relative to one another}, and the volume of the sphere will increase. This is exactly what
we expect. The Ricci curvature is interpreted as the second derivative of the volume of a region of space defined by
test particles in this way. The fact that the second derivative is positive rather than negative tells us that we
are observing the kind of repulsion provided by the cosmological constant, not the attraction that results from the
existence of material sources.
\end{eg}


\begin{eg}{Schwarzschild-de Sitter space}\label{eg:sch-de-sitter}

% http://en.wikipedia.org/wiki/De_Sitter%E2%80%93Schwarzschild

The metric
\begin{equation*}
  \der s^2 = \left(1-\frac{2m}{r}-\frac{1}{3}\Lambda r^2\right)\der t^2 - \frac{\der r^2}{1-\frac{2m}{r}-\frac{1}{3}\Lambda r^2} - r^2\der\theta^2-r^2\sin^2\theta\der\phi^2
\end{equation*}
is an exact solution to the Einstein field equations with cosmological constant $\Lambda$, and can be interpreted as a universe in which the
only mass is a black hole of mass $m$ located at $r=0$. Near the black hole, the $\Lambda$ terms become negligible, and this is simply
the Schwarzschild metric. The characteristic scale of the black hole, e.g., the radius of its event horizon, is still set by $m$, so
we can see that cosmological expansion does not affect the size of gravitationally bound systems on smaller scales.
\end{eg}
% sch_de_sitter.mac

<% end_sec %> % The vacuum-dominated solution

<% begin_sec("The matter-dominated solution") %>\label{sec:matter-dominated}
Our universe is not perfectly vacuum-dominated, and in the past it was even less so. Let us consider the matter-dominated
epoch, in which the cosmological constant was negligible compared to the material sources. 
The equations of state
for nonrelativistic matter (p. \pageref{eg:dust-and-radiation-cosm}) are
\begin{align*}
 P&=0 \\
 \rho &\propto a^{-3}
\end{align*}
so the Friedmann equations become
\begin{align*}
  \frac{\ddot{a}}{a}   \quad          &=  - \frac{4\pi}{3}\rho \\
  \left(\frac{\dot{a}}{a}\right)^2    &= \frac{8\pi}{3}\rho-k a^{-2} \qquad ,
\end{align*}
where for compactness $\rho$'s dependence on $a$, with some constant of proportionality, is not shown explicitly.
A static solution, with constant $a$, is impossible, and $\ddot{a}$ is negative, which we can interpret semiclassically
in terms of the deceleration of the matter in the universe due to gravitational attraction. There are three cases
to consider, accoding to the value of $k$.

<% begin_sec("The closed universe") %>
We've seen that $k=+1$ describes a universe in which the spatial curvature is positive, i.e., the circumference
of a circle is less than its Euclidean value. By analogy with a sphere, which is the two-dimensional surface of constant positive curvature,
we expect that the total volume of this universe is finite.

The second Friedmann equation also shows us that at some value of
$a$, we will have $\dot{a}=0$. The universe will expand, stop, and then recollapse, eventually coming back together
in a ``Big Crunch'' which is the time-reversed version of the Big Bang. 

Suppose we were to describe an initial-value problem in this cosmology, in which the initial conditions are
given for all points in the universe on some spacelike surface, say $t=\text{constant}$. Since the universe
is assumed to be homogeneous at all times, there are really only three numbers to specify, $a$, $\dot{a}$, and $\rho$:
how big is the universe, how fast is it expanding, and how much matter is in it?
But these three pieces of data may or may not be consistent with the second Friedmann equation. That is, the
problem is overdetermined. In particular, we can see that for small enough values of $\rho$, we do not have
a valid solution, since the square of $\dot{a}/a$ would have to be negative. Thus a closed universe requires
a certain amount of matter in it. The present observational evidence (from supernovae and the cosmic microwave
background, as described above) is sufficient to show that our universe does not contain this much matter.
<% end_sec %> % The closed universe

<% begin_sec("The flat universe") %>
The case of $k=0$ describes a universe that is spatially flat. It represents a knife-edge case lying between the
closed and open universes. In a semiclassical analogy, it represents the case in which the universe is moving
exactly at escape velocity; as $t$ approaches infinity, we have $a \rightarrow\infty$, $\rho\rightarrow 0$, and
$\dot{a}\rightarrow 0$. This case, unlike the others, allows an easy closed-form solution to the motion.
Let the constant of proportionality in the equation of state $\rho \propto a^{-3}$ be fixed by setting
$-4\pi\rho/3=-ca^{-3}$. The Friedmann equations are
\begin{align*}
  \ddot{a} &= -ca^{-2} \\
  \dot{a} &= \sqrt{2c} a^{-1/2} \qquad .
\end{align*}
Looking for a solution of the form $a\propto t^p$, we find that by choosing $p=2/3$ we can simultaneously
satisfy both equations. The constant $c$ is also fixed, and we can investigate this most transparently
by recognizing that $\dot{a}/a$ is interpreted as the Hubble constant, $H$,\index{Hubble constant}
which is the constant of proportionality relating a far-off galaxy's velocity to its distance.
Note that $H$ is a ``constant'' in the sense that it is the same for all galaxies, in this particular model
with a vanishing cosmological constant; it does not stay
constant with the passage of cosmological time.
Plugging back into the original form of the Friedmann equations, we find that the flat universe can
only exist if the density of matter satisfies $\rho=\rho_{crit}=2H^2/8\pi=2H^2/8\pi G$.
The observed value of the Hubble constant is about $1/(14\times10^9\ \text{years})$, which is roughly
interpreted as the age of the universe, i.e., the proper time experienced by a test particle since
the Big Bang. This gives $\rho_{crit}\sim 10^{-26}\ \kgunit/\munit^3$.

% G=6.7 10^-11 m3kg-1s-2
% t=14 10^9 * 365.25 *24 * (3600 s)
% H=1/t
%  2H^2/(8piG)
%    6.08487033002423*10^-27 kg/m3

<% end_sec %> % The flat universe

<% begin_sec("The open universe") %>
The $k=-1$ case represents a universe that has negative spatial curvature, is spatially infinite, and is also infinite in time,
i.e., even if the cosmological constant had been zero, the expansion of the universe would have had too little matter in it
to cause it to recontract and end in a Big Crunch.


<% end_sec %> % The open universe

<% end_sec %> % The matter-dominated solution

<% begin_sec("Observation") %>
Current observational evidence indicates that we live in an open universe. Historically, it was very difficult
to determine the universe's average density, even to within an order of magnitude. Most of the matter in the universe
probably doesn't emit light, making it difficult to detect. Astronomical distance scales are also very poorly calibrated
against absolute units such as the SI. The observation of the universe's accelerating expansion, however, marked the beginning
of a new era of high-precision cosmology. If the universe's density had been significantly higher than the critical density, then
it would have begun recontraction before it could enter the vacuum-dominated phase of accelerated expansion. We therefore
have $\rho \lesssim \rho_{crit}$.

A further constraint on the models comes from accurate measurements of the cosmic microwave
background, especially by the 1989-1993 COBE probe, and its 2001-2009 successor, the Wilkinson Microwave Anisotropy Probe,
positioned at the L2 Lagrange point of the earth-sun system, beyond the Earth on the line
connecting sun and earth.
The temperature of the cosmic microwave background radiation is not the same in all directions, and
its can be measured at different angles.
In a universe with negative spatial
curvature, the sum of the interior angles of a triangle is less than the Euclidean value of 180 degrees.
Therefore if we observe a variation in the CMB over some angle, the distance between two points on
the surface of last scattering is actually greater than would have been inferred from Euclidean geometry.
The distance scale of such variations is limited by the speed of sound in the early universe, so
one can work backwards and infer the universe's spatial curvature based on the angular scale of the
anisotropies.
<% marg(60) %>
<%
  fig(
    'cmb-geometry',
    %q{The angular scale of fluctuations in the cosmic microwave background can be used to infer the curvature of the universe.}
  )
%>
<% end_marg %>


Astrophysical considerations provide further constraints and consistency checks. In the era before the advent
of high-precision cosmology, estimates of the age of the universe ranged from 10 billion to 20 billion years, and the
low end was inconsistent with the age of the oldest globular clusters. This was believed to be a problem either for
observational cosmology or for the astrophysical models used to estimate the age of the clusters: ``You can't be
older than your ma.'' Current data have shown that the low estimates of the age were incorrect, so consistency is
restored.

Another constraint comes from models of nucleosynthesis during the era shortly after the Big Bang (before the
formation of the first stars). The observed relative abundances of hydrogen, helium, and deuterium cannot be
reconciled with the density of ``dust'' (i.e., nonrelativistic matter) inferred from the observational
data. If the inferred mass density were entirely due to normal ``baryonic'' matter (i.e., matter whose mass
consisted mostly of protons and neutrons), then nuclear reactions in the dense early universe should have
proceeded relatively efficiently, leading to a much higher ratio of helium to hydrogen, and a much lower
abundance of deuterium. The conclusion is that most of the matter in the universe must be made of
an unknown type of exotic non-baryonic matter, known generically as ``dark matter.''\index{dark matter}
<% end_sec %> % observation

<% end_sec %> % Cosmological solutions

<% end_chapter %>
